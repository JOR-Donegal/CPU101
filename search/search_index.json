{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>Background to General Computing</p> <p>My notes do not have prerequisites, you may not have previously studied computer architecture, or programming. You may not have any theoretical background in operating systems. I intend to lightly cover these topics here. These notes are an update of notes I wrote for undergrad Computer Arctitecture last from c.2014-2017.</p> <p>Assisted computation has existed at least from the time of the abacus, but until the era around the second world war, the utility of devices was restricted by the available technologies [1]. This did not prevent advances in the underlying mathematics; names like Boole, Babbage, Lovelace, Napier, Turing, Shannon and Von Neuman are known even to the public and no further examination of their contributions will be made here. From the twentieth century, there was a rapid development in the technologies used for data communications, data storage and processing. Mechanical technologies were used for computing and automation, with relays (an electrical switches) being common in both. Mechanical computing gave way to the first generation of electronic computers based on vacuum tubes or thermionic valves [2]. Preventive maintenance schemes were developed on an ad-hoc basis to keep these valve systems operational, and the first generation of commercial computers were delivered in the early 1950s.</p> <p>The development of solid-state devices such as the transistor in 1947 [3] led to the second generation of computing technology. Transistors were smaller and required less power and as manufacturing techniques improved, so did individual component service lifetime. Military specification transistors may have had mean time between failure (MTBF) of &gt;10^9 hours at 50\u2070C and this figure of 10^9 hours for transistors still shows in many textbooks as a rule of thumb. By the early 1950s, the first transistor computers were being developed at the University of Manchester [4] and commercial computers were introduced in the late 1950s. These were mainframes and (much later models) are still found today in banking, insurance and large-scale manufacturing.</p> <p>As early as 1949, Werner Jacobi of Siemens AG filed a patent [5] for an amplifier with three transistors, the first integrated circuit (IC). Later work by Jack Kilby of Texas Instruments [6] created an integrated oscillator circuit; Kilby was awarded the Nobel prize in physics in 2000 for his contributions. If ICs were produced at volume, the cost per transistor, size and power utilization all dropped, but so did circuit complexity and assembly effort. The introduction of ICs is typically used to define the third generation of computing hardware in the 1960s. Minicomputers emerged and became ubiquitous in even small universities. This availability of computers led to an increase in the pace of development across many related fields. I first worked on an Apple II around 1978! As each new generation of computer emerged (VIC20, Commodore 64, Spectrum) I learned a little bit more. My first work with mini-computers was with Honeywell Level 6 minicomputers; the Central Processing Unit (CPU) was a pizza-box sized motherboard using ICs. The OS was configured using automation (!) and was built using a Multics mainframe.</p> <p>The ubiquity of computers is something that only emerged in the past generation, and I would argue that the entire field remains immature, we have much to learn.</p> <p>In these notes, I want to compensate for the misunderstanding many people have technology. Younger people are often referred to as digital natives, they've grown up with technology. But they've grown up as operators of technology. As that technology gets more complex, the level of abstraction that somebody interfaces with is further away from how things actually work inside the box. In the next few pages of notes, I want to go back and work my way forward from the kind of electronic components that have been available since around 1947, as far as (conceptually) how to build a CPU. This knowledge is a prerequisite for anyone working with computer hardware or with low level computer languages or drivers.</p>"},{"location":"#recommended-book","title":"Recommended Book","text":"<p>I am going to make an unusual recommendation here. If PC hardware is new to you I am going to suggest a book for practitioners of computer build and repair rather than the sort of book used by academics. Upgrading and Repairing PCs by Scott Mueller has been around for years and is relatively easy reading, if perhaps getting outdated.  </p>"},{"location":"#references","title":"References","text":"<p>[1] M. Guarnieri, \"The Roots of Automation Before Mechatronics [Historical],\" IEEE Industrial Electronics Magazine, vol. 4, no. 2, pp. 42-43, 2010, doi: 10.1109/MIE.2010.936772. [2] G. R. Jessop, \"Developments in thermionic valves during the last sixty years,\" Journal of the Institution of Electronic and Radio Engineers, vol. 57, no. 2, pp. 81-90, 1987, doi: 10.1049/jiere.1987.0034. [3] \"The transistor \u2014 A new amplifier,\" Electrical Engineering, vol. 67, no. 8, pp. 740-740, 1948, doi: 10.1109/EE.1948.6444253. [4] \"Transistor computers,\" Journal of the Institution of Electrical Engineers, vol. 2, no. 21, pp. 542-543, 1956, doi: 10.1049/jiee-3.1956.0240. [5] W. Jacobi, \"Halbleiterverst\u00e4rker,\" Germany, 1952. [ ] J. Kilby, \"Miniaturized Electronic Circuits,\" USA, 1964. [6] J. Kilby, \"Miniaturized Electronic Circuits,\" USA, 1964.</p>"},{"location":"a/","title":"Components","text":"<p>Modern devices abstract way thier internal complexity. I want to try to explain how a computer works from the component level upwards.  These notes are extracted from undergraduate Computer Architecture material I wrote c. 2014. </p> <p>The fundamental components we use in every electrical circuit include:</p>"},{"location":"a/#resistors","title":"Resistors","text":"<p>Resistors are two-terminal devices that restrict, or resist, the flow of current. The larger the resistor the less current can flow through it for a given voltage as demonstrated by Ohm\u2019s law: V= I*R. </p> <p>Electrons flowing through a resistor collide with material in the resistor body, and it is these collisions that cause electrical resistance. These collisions cause energy to be dissipated in the form of heat or light (as in a toaster or an old incandescent light bulb). </p> <p>Resistance is measured in Ohms (\u03a9) and an ohm is defined by the amount of resistance that causes 1A of current to flow from a 1V source. </p> <p>The amount of power (in Watts) dissipated in a resistor can be calculated using the equation P= I*V = I<sup>2</sup>R A resistor that can dissipate about 5 Watts of power would be about the size of a pen and a resistor that can only dissipate 1/8 Watt is about the size of a grain of rice. </p> <p>If a resistor is placed in a circuit where it must dissipate more that its intended power, it will melt!</p> <p>A standard colour coding scheme exists for resistors and any experienced electronics person can tell the value of a resistor at a glance! However, as the number of colur bands can vary, it\u2019s always better to check with a digital volt meter (DVM).</p> <p>Look up resistor colour coding now.</p>"},{"location":"a/#capacitors","title":"Capacitors","text":"<p>A capacitor (called a condenser in the old days!) is a passive two-terminal device that can store electric energy in the form of charged particles. A capacitor is like a reservoir of charge that takes time to fill and empty. </p> <p>The construction of capacitors is more varied than that or resistors, but the general principle is that two conductive plates are separated by is non-conductive dielectric.</p> <p>When there is a potential difference (voltage) across the conductors, a static electric field develops across the dielectric, causing positive charge to collect on one plate and negative charge on the other plate. </p> <p>Energy is stored in the electrostatic field.</p> <p>The voltage across a capacitor is proportional to the amount of charge it is storing, the more charge added to a capacitor of a given size, the larger the voltage across the capacitor. It is not possible to instantaneously move charge to or from a capacitor, so it is not possible to instantaneously change the voltage across a capacitor. It is this property that makes capacitors useful in many applications.</p> <p>Capacitance is measured in Farads. A one Farad capacitor can store one Coulomb of charge at one volt. For engineering on a small scale (i.e., hand-held or desk-top devices), a one Farad capacitor stores far too much charge to be of general use (it would be like a car having a 1000 gallon petrol tank). </p> <p>More useful capacitors are measured in micro-farads (uF) or pico-farads (pF). The terms \"milli-farad\u201c and \"nano-farad\" are rarely used. Large capacitors often have their value printed plainly on them, such as \"10 uF\" (for 10 microfards).</p>"},{"location":"a/#inductors","title":"Inductors","text":"<p>An inductor (also called a choke or coil) is a passive two-terminal electrical device that stores energy. Although this sounds a bit like a capacitor, it is different in that a capacitor stores energy in an electric field, an inductor stores it in its magnetic field.  In practical terms, inductance is the characteristic of an electrical circuit that opposes the starting, stopping, or a change in value of current. Even a perfectly straight length of wire has some inductance. Current flowing in a conductor produces a magnetic field surrounding the conductor; when the current changes, the magnetic field changes. This causes a relative motion between the magnetic field and the conductor, and a back electromotive force (EMF) is induced in the conductor. The polarity of the back electromotive force is in the opposite direction to the applied voltage of the conductor. The overall effect will be to oppose a change in current magnitude. Some effects we may notice are that the start-up current drawn by an electric motor is much higher than the current required to run the motor (the coil has to \u201ccharge\u201d).   </p> <p>The symbol for inductance is L and the basic unit of inductance is the HENRY (H). To quantify the unit, an inductor with an inductance of 1H produces an EMF of 1V when the current through the inductor changes at the rate of 1A per second.</p> <p>In appearance, an inductor can be as simple as a coil of copper wire. They can be cylindrical-shaped or torus shaped (the engineer\u2019s term for shaped like a doughnut!)</p> <p>More commonly, an inductor will have a core of a ferromagnetic material.  The core material has the effect of increasing the magnitude of the magnetic field and reducing its physical size.</p>"},{"location":"a/#diodes","title":"Diodes","text":"<p>Very frequently we need electronic components which can act like one way valves; current will only pass one way through the device, it will not pass back. These devices are called diodes. They are two terminal devices and are used for a range of purposes. They consist of two layers of a semiconductor (normally silicon) sandwiched together. </p> <p>Almost every power supply will have a few diodes combined with a transformer and a few capacitors.</p> <p>One version of the diode is the LED or light emitting diode. A property of the joint between the two layers causes light to be emitted. </p>"},{"location":"a/#transistors","title":"Transistors","text":"<p>A transistor is an active electronic device with (at least) three terminals. They generally consist of three layers of a semiconductor (normally silicon) sandwiched together. </p> <p>A transistor can act like an amplifier. A voltage or current applied to an input control terminal to change the current flowing through another pair of output terminals; this property is called gain. Because the controlled power can be higher than the controlling power, a transistor can amplify a signal. This is the basis for the audio subsystems of all amplifiers, stereo systems, iPods, etc.</p> <p>A transistor can act also like a switch. When no voltage is applied to an input control terminal, no current flows through another pair of output terminals. Alternatively, when a voltage over a particular threshold is applied to the input control terminal, then current flows through the output terminals.  This is the basis for digital logic and all modern binary computers.</p> <p>Because the controlled power can be higher than the controlling power, a transistor can switch a large current using only a small controlling current. This is the basis of all power electronics.</p> <p>The original transistors we used were called bipolar transistors. </p> <p>These are three terminal active devices that can conduct current between two terminals (the collector and the emitter) when a third terminal (the base) is driven by an appropriate signal.</p> <p>The transistor switches used in modern digital circuits are called \u201cMetal Oxide Semiconductor Field Effect Transistors\u201d, or MOSFETs or just FETs. FETs are also three terminal devices that can conduct current between two terminals (the source and the drain) when a third terminal (the gate) is driven by an appropriate logic signal. FETs can be thought as electrically controllable ON/OFF switches</p>"},{"location":"b/","title":"Logic Gates","text":"<p>Once we have basic components, we can build for complex circuits, logic gates.</p> <p>The study of logic goes back (at least!) to our earliest cultural origins; the Arabs, Greeks, Chinese and Indians all had advanced concepts which were the origin of our modern science.  In modern computer science and mathematics, we trace our reasoning back to Aristotle and the early Greek mathematicians, as we strove to formally describe reasoning, arguments and proofs in an unambiguous mathematical language. The term logic itself comes from the Greek \u03bb\u03bf\u03b3\u03b9\u03ba\u03ae logik\u0113 or the study or arguments. </p> <p>Some of the most important work in logic (in fact the basis of modern computer theory) was carried out by George Boole at UCC from 1849. Boole established the system now known as Boolean algebra, where variables are truth values true or false. The device which we based our electronic revolution on was the transistor; electronic switches which could have the values on or off. Boolean algebra was ideally suited to this binary environment has been fundamental to our development of digital logic, modern electronics, and eventually computing. In the 1930\u2019s, Claude Shannon applied Boolean algebra to circuits built using switches, providing the foundations for modern computing.</p> <p>Boolean expressions are normally true or false, or on and off, or 1 and 0, all are representations of the same thing. In any system, we can have Boolean variables which can contain a Boolean expression. </p> <p>Boolean operators let us look at the relationship between the state of these variables and an output. Let\u2019s go with variable C for an output.  We know that if A is on (true) and B is on (true) then the machine is safe and C should be equal to on (true) But look at all the possible cases that give rise to an output. We call this a truth table; it allows us to look at all possible inputs and have a predictable and desired output. </p> Fig 1. An AND gate, truth table and symbol. <p>In logic terms, IF A AND B THEN C. </p> <p>This behaviour is the Boolean operator AND. This is formally represented by the symbol and truth tables shown in Fig 1. By convention, we show our Boolean expressions as 1 and 0, the binary levels used in both digital logic and in computing.</p> <p>The second Boolean operator we will consider is the OR gate, Fig 2. In this case if either input A or input B is equal to 1 (true) then the output of the gate will be 1 (true).</p> <p>The output is 0 (false) only when both inputs are 0 (false).</p> Fig 2. An OR gate, truth table and symbol. <p>In electronics, we may sometimes buffer an input from an output. This is done for a range of reasons, mostly to reduce the electrical load on a circuit.  </p> <p>The triangle symbol on its own means it\u2019s just a buffer, it does not change the Boolean value at its input to anything different at the output.</p> Fig 3. A buffer, truth table and symbol. <p>Sometimes we need a logic function which just inverts a Boolean variable. For example if we have a 1 and we need a 0, or vice versa. We refer to this as a NOT function.</p> <p>We can signify this in a few different ways. We can use any standard logic symbol with a circle on the output. The circle denotes an inversion.  Alternatively, we can put a bar over the variable symbol; a bar over A means NOT A. </p> Fig 4. An inverter, truth table and symbol. <p>We can apply the same sort of approach to other logic gates. For example a NOT AND gate is a NAND gate. It has the same truth table as an AND gate, except with the output inverted. Similarly a NOT OR gate is called a NOR gate.</p> Fig 5. NOR and NAND, truth table and symbol. <p>The final Boolean operator we need to look at is the exclusive OR (XOR) gate. This is a special case or the OR gate where the output is 1 if either input is 1, but is 0 is all inputs are 1. Obviously we can also have an XNOR, which is an XOR with the output inverted. </p> Fig 6. XOR and XNOR, truth table and symbol. <p>In early digital systems (c. 1950) we actually built logic gates out of transistors, resistors and other discrete components. </p> <p>By the 1960\u2019s we had integrated these transistors into single chunks of silicon we called integrated circuits or IC\u2019s. This allowed for the next revolution in computing, which allowed us to get a CPU onto a single large circuit board. These early silicon chips used transistor-transistor logic (TTL) and can be recognised by a 74xxx prefix. </p> <p>For example, a 7400 was a package with 14 legs which had four NAND gates inside.</p> Fig 7. 7400 TTL. <p>In later circuits, the 74xxx series were superceeded by 4xxxx series, using CMOS transistors and lower power.</p> <p>Based on the availability of the early silicon chips, computers began to be built which were commercially viable. The IBM 360 series revolutionised early computing using these kinds of technologies. Throughout the 1970\u2019s mini-computers and mainframes were developed and the computing industry blossomed.</p> <p>I still use these chips when designing and building simple prototypes.</p> <p>Do some background reading and identify some circuits that use transistors to implement logic gates.</p>"},{"location":"c/","title":"Memory","text":"<p>In this section, I am going to build up gates into useful subsystems, first memory!</p>"},{"location":"c/#the-setrest-latch","title":"The Set/Rest Latch","text":"<p>The first thing that we needed to do useful calculations with these gates was to have a way of storing numbers (remember, all numbers are represented in binary within the computer); this is the concept of memory. We need a circuit which can store a single bit of data; we call a circuit like this a latch. There are several types of latch available in TTL which were built into early computers (and are now integrated into much larger circuits).</p> Fig 8. SR Latch. <p>One of these simple circuits was the set-reset latch or S-R Latch. Very simple, if you raise the input S to a one, Q will set to 1 and \u00afQ will reset to 0. If you raise the R input to 1, Q will reset to 0 and \u00afQ will set to 1. Work your way through the circuit to confirm this!</p> <p>One of the problem with this circuit is that if you raise S and R to 1 at the same time, the circuit acts in an unpredictable manner. Again, try working your way through the circuit to see.</p> <p>Up to now, all the digital gates and circuits we have looked at have been deterministic, that means that for the inputs given, there is only one possible outcome. Once we start dealing with circuits which can store bits (memory) we need to understand the concept of state; a circuit can be in a particular state before we encounter it and the output of any action may depend on the state the circuit was in before we started.</p>"},{"location":"c/#clocks-and-timing","title":"Clocks and Timing","text":"<p>In a real computer, most things are synchronised by a timing signal. Have a look at the circuit below where we introduce a timing signal, known as a clock. The AND gates block any inputs from either S or R until the clock signal is high. Now we can control when the set or reset command are sampled.</p> Fig 9. Clocked SR Latch."},{"location":"c/#the-d-latch","title":"The D Latch","text":"<p>In a D Latch, we allow only one input, directly to the set AND gate at the top of the diagram, and via an inverter to the reset AND at the bottom of the diagram. When D=1 and the clock signal is high, we set Q=1. When D=0 and the clock signal is high, we reset Q=0. Work your way through the circuit to confirm this. This is a pretty good circuit but there is one issue. The clock pulse is quite wide and the latch can be set at any stage while the pulse is high. This is none too precise, especially when we are trying to get the maximum speed and performance out of our electronics. </p> Fig 10. D Latch."},{"location":"c/#edge-detection","title":"Edge Detection","text":"<p>One of the ways we can get best performance out a clocked circuit is to activate the circuit or the edge of the pulse rather than on the pulse itself. Consider the circuit below. Each gate has a propagation delay to activate. Suppose each gate takes 1 ns to activate and a starts as 0. If a is at 0 then b must be at 1 because of the inverter. When the pulse goes high at a it goes high at c at the same time. However, the inverter causes a delay of 1 ns before b becomes 0. For this brief period, both b and c were 1 and the AND gate output becomes 1 for this brief time period. We have detected the rising edge of the pulse.</p> Fig 11. Edge Detection. <p>One other interesting thing we can do here is to detect both the rising and falling edge. That would give us a good narrow pulse at double the rate of the clock. I wonder where we might come across Double Data Rate (DDR) being used. Try an Internet search if you haven\u2019t come across this before.</p> <p>We can use an edge trigger circuit to trigger a D Latch to make it much more precise. In the circuit below, the clock is intercepted by a trigger circuit to make an edge-trigger circuit. A latch which is edge triggered is called a flip-flop.</p> Fig 12. Flip-flop. <p>Flip-flops are very commonly used in all kinds of digital logic circuits. There are several different block circuits used. To keep things simple, we abstract the details away. We can treat a half adder like a black box, not worrying too much about what is inside it. This allows us to simplify our diagrams, getting more and more unnecessary detail out of the way so we can understand more complex circuits more easily.</p> <ul> <li>A D Latch is triggered by a positive clock</li> <li>A D Latch is triggered by a negative clock</li> <li>The triangle indicates a positive edge triggered flip-flop</li> <li>The triangle and circle indicates a negative edge triggered flip-flop</li> </ul> Fig 13. Triggering. <p>By combining four flip-flops in an array, we can create a four bit memory store, a store which could hold half a byte of information. We could use this as memory, or we could use it as a temporary area to store data that we need to process; we call this a register.</p> Fig 14. 4 bit memory. <p>When the circuit starts up, everything is at zero. I set values for D0 to D3 on the input bus. When the values are set and I want to write to the register, I toggle the write switch. This enables the clock through the AND gate and creates an edge to clock the D flip flops. They then store whatever is on the input bus. The output is available on the output bus indefinetely or until it is overwritten or the power is removed. </p> <p>As with all things computing, once we understand how something complex works inside, we build an abstraction to hide the detail. A block diagram of a register might look something like this.</p> Fig 15. 8 bit register. <p>Some registers have a very defined purpose and will have extra functionality built into their logic. For example, there are times when we need sequential counters, these may have functions like increment or decrement built in. Some mathematical manipulations require us to shift the bits in a register left or right, this may also be built in. </p>"},{"location":"d/","title":"Calculation","text":"<p>We now know how to do basic digital logic and how to create basic memory elements. The next thing we need to be able to do is to manipulate numbers stored in those memory elements.</p> <p>When we add two binary numbers together, what are the possible outcomes?</p> Table 1. Simple addition. <p>So to add two binary numbers, we will need outputs for a sum and a carry. Let\u2019s do a truth table and figure out how this should work.</p> Table 2. Simple addition, truth table. <p>Make sure this makes sense to you before reading on! Can you figure out what gate matches the sum column\u2026what gate give a 1 output when either input is one, but a 0 output when both gates are 1?  You should know this!  Look at the carry column. What gate give a 1 output only when both inputs are 1?  If you couldn\u2019t figure this out, go back and read the notes on logic gates again.</p> <p>A circuit to perform this simple mathematical function is called a half-adder (for reasons we will see later!). I have built a half-adder circuit to test it. I have used two memory elements (D flip flops) to store my A and B values. I have included the XOR and AND gates and have put in some lights so I can see what is going on. I tested it and it works!</p> fig 16. A half adder. <p>So we have a success\u2026.we can now add any numbers up to\u2026..2! Check closely, we can never have an answer of 3.</p> <p>This isn\u2019t much use. When you did binary in semester 1, you learned that to add two numbers, you need to be able to carry from one stage to the next. In the adjacent sum;</p> <ul> <li>1 + 0 = 1 and no carry</li> <li>1 + 1 + no carry = 0 carry the 1</li> <li>1 + 1 + 1 carry = 1 carry the 1</li> <li>1</li> </ul> <p>Giving an answer of 1101</p> <p>Our half-adder would work fine for the first calculation, where we can only add A and B. However, in the second calculation, we must be able to add A and B and the carry. What would a truth table for this look like?</p> <p>I have built a (slightly more complex!) circuit in Cedar Logic to allow for a carry-in. What I have done in this case is to put two half adders together, one adds the two numbers to get the sum, the other adds the carry in to the sum to get a final sum.  Now we can add 1 + 1 + 1 to get 3!</p> fig 17. A full adder. <p>Take a look at the truth table. </p> Table 3. Full adder addition, truth table. <p>Finally, do a little reading on twos complement in binary, we can use that technique in maths to do subtraction. </p> <p>I have created a simple circuit using four full adders which adds two four bit numbers in the normal way as A + B. However if I turn on the subtract switch, it adds one (by connecting to the carry in) and inverts the B bits giving the sum A-B. </p> fig 18. A four bit adder. <p>By using simple tricks and mathematical techniques, we can do complex manipulation with very simple circuits.</p> <p>This may not seem very impressive, but remeber the first microprocessor was the Intel 4004, a four-bit processor!</p>"},{"location":"e/","title":"Processor Architecture","text":"<p>The key component in any computer is the Central Processor Unit or CPU. This does all the calculating and co-ordination for the entire computer. A CPU is normally an integer processor. To do floating point calculation, we need to use tricks or a dedicated coprocessor.</p> <p>In previous notes we have seen how basic memory elements work (latches and flip-flops). On their own, this would not be very useful. But we can combine several memory elements together to create a register, a very fast but very small memory store. If we wanted to do a calculation, we could put numbers in two registers and if we knew how to do so, we could add them together and put the result in a third register. </p> <p>We have also seen how we can use simple circuits to add two numbers together. In Boolean logic, that if you can add, you can multiply using successive addition. We can also use some Boolean maths to make essentially the same circuit do subtraction. If we can do subtraction, we can also do division by successive subtraction . So our simple adder can be quite a powerful calculating circuit. In a CPU the circuitry to do these calculations is referred to as an Arithmetic Logic Unit (ALU).</p> <p>Before we could make a useful calculating machine, we need some way to tie all of these things together. The pathways which allow parts of a computer to communicate with other parts of the computer are called a bus. It would be nice if we knew when things worked or didn\u2019t work, if we had some status flags to indicate when a calculation fails or returns a number that is too big. The final thing we are going to need is some control logic to hang all these things together and to synchronise everything we want to happen. So here is my design for a simple CPU, (fig19)!</p> fig 19. An ALU block diagram. <p>You already know how to build one of these! We know how to make a register and we know how to make an adder/subtractor. So all we really need to do is to hang it all together. Below there is a diagram of a 4 bit adding/subtracting machine. You may think this is not very impressive, but remember, the world\u2019s first micro-processor was an Intel 4004, a 4 bit processor built for early calculators.</p> fig 20. An ALU block diagram. <p>There are a few differences between our simple CPU and a real, usable CPU/computer. Most computers will require more functionality than add and subtract. For us to do repetitive addition, we would need to build additional circuitry. We could load the number to be multiplied into register A and the number it is to be multiplied by in register B. Then we would need the digital logic to add A to A, B number of times. Not a lot of circuitry, but more than we can cover in a brief course. For every instruction which we add in hardware, we make the CPU bigger, more expensive, more complex, and more power-hungry. There has been a debate for decades now as to which is the correct approach. Processors which have few instructions but are fast and cheap are called Reduced Instruction Set Computers or RISC. Expensive processors which are billions of transistors and hundreds of instructions are call Complex Instruction Set Computers or CISC. All modern PCs use CISC processors. All modern smartphones use RISC. Tablets are a mix, Apple/Android all use RISC, some Windows tablets use CISC.</p> <p>In a real computer, we need somewhere to keep our instructions (or programmes) and our data. This is main memory and we will look at that next.  </p>"},{"location":"f/","title":"Digital Logic and Memory","text":"<p>The first computer systems (e.g. ENIAC) required programmes to be hard wired, literally with jumper cables, dials and switches. A computer at the time looked like something out of a bad sci-fi movie!  </p> <p>From 1944 and the development of the EDIVAC computer, work began on holding computer programmes and data in electronics memory. In 1945 Jon Von Neumann proposed that a working computer system should consist of a processing unit or ALU, a control unit, and memory. We have looked at the ALU and at control in previous notes, let\u2019s take a look at memory!</p> <p>Firstly some terminology.  A bit is a single binary digit and can be a one or a zero A nibble is a group of 4 bits A byte is a group of 8 bits A word is a group of 16 or more bits, we can have 32 bit words, 64 bit words etc.</p> <p>In previous notes, we have built the bones of a simple 4 bit computer. Can we provide this computer with a small amount of 4-bit memory?</p> <p>We know that we could store bits in a register, so building a memory element for a single 4 bit nibble will be quite easy. In fact, the design is that of a 4 bit register. As previously, we will clock the circuit. When write is enabled, whatever is on the input bus will be copied to the registers. They can then be read at the output bus an unlimited number of times. Once write enabled is switched off, any changes in the input bus will not be copied to the memory element. So now we have a single memory element, good to store a single nibble of data, fig 21. </p> Fig 21. 4 bit register. <p>How do we scale that up to make a useful quantity of memory?</p> <p>When we design memory, we tend to do so as a grid of rows and columns. Imagine a spreadsheet where we lay out the memory. We have a 4 bit computer, each column could be one 4 bit nibble and we can populate the grid with ones and zeros.</p> Table 4. Data in 4x3 memory. <p>We can only read/write one nibble at a time, so we are going to need to be able to select which nibble we want to read from. Let\u2019s call that the address of the nibble of data. So we need some way to select the correct address.</p> Table 5. Addressing in 4x3 memory. <p>If we duplicate this column, we can create a new memory address (nibble) of data per column. Only one column should be active at a time, so all the outputs of all the other columns will be zero. If we OR all the outputs for each bit in the nibble, we will only get the output from the enabled column.</p> Fig 22. 4x3 memory. <p>We have one more step to look at. There is a major scalability problem with this circuit! We need an address line per address.</p> <p>Imagine if we had a 256 addresses for data, we would need 256 lines on an address bus! But we can count from 0-255 using only eight bits (2<sup>8</sup>=256). Wouldn\u2019t it be more useful to have an address bus where we carried only the binary number of the address and then translated it into an enable line at the memory array?</p> <p>This type of circuit would be called a 2-to-4 line decoder. It would be typical of the kind of logic on the memory bus of a computer to allow the CPU to select exactly which memory address it wanted to read or write to/from.</p> <p>On a 32 bit computer we need 32 address line to address 4,294,967,296 memory locations!</p> Fig 23. 2-4 line decoder. <p>Fig 24 is our finished circuit, including the memory decoder. We only have 3 address available in memory, so we have one spare address enable line.</p> Fig 24. 2-4 line decoder. <p>This is a very small example, but it gives you an idea of how to construct a kind of memory called Static Random Access Memory or SRAM. This example is too small to be practical, but it gives you an idea how you might build working memory. As you can see, there are many components per bit, so this kind of memory is very expensive and is used for specialist purposes only, mostly for cache, which we will deal with in a later lecture. </p> <p>Most of the memory in a conventional computer is made of fewer components, perhaps two transistors and a capacitor per bit.But the row and column selection logic will be exactly the same. This kind of memory is called Dynamic Random Access Memory or DRAM, and we will see this in detail later in the module.</p> <p>As with all things complex, once we understand how memory works and what the terminology is around it, we can create an abstraction to simplify diagrams and make it easier to get an overview of what is going on. A memory block might be shown as something Fig 25.</p> Fig 25. Block diagram. <p>A final word...</p> <p>An architecture where programmes and data share memory is still called a Von Neumann architecture. There is another option, the Harvard architecture. In this type of computer, the memory which holds programmes is completely separate from the memory to hold data. From a security perspective, there may be much to be said for this.   </p>"},{"location":"g/","title":"Modern CPU Design","text":"<p>In previous notes we have seen how basic memory elements work (latches and flip-flops). On their own, this would not be very useful. But we can combine several memory elements together to create a register, a very fast but very small memory store. We have also seen how we can use simple circuits to add two numbers together and we have built and manipulated an Arithmetic Logic Unit or ALU. We have taken a look at main memory and at this stage you should understand how memory stores data in fixed sizes (nibble, bytes or words) in unique addresses. With these elements, we have the basic ingredients, but we are missing the key elements to tie it all together to make a workable computer system. Let\u2019s do that now!</p> <p>Firstly, we need to get an idea of how a CPU functions. It\u2019s actually very simple and it\u2019s called the fetch-execute cycle; or sometimes the fetch-decode-execute cycle (Fig 26). A CPU runs like clockwork (literally, everything runs off an electronic pulse called a clock). A computer programme is a list of instructions which the CPU will run through in sequence. </p> Fig 26. fetch-decode-execute cycle . <p>Let\u2019s imagine we have a programme with 20 steps, starting at step zero.  We need to keep track of where we are in the sequence and to do that we have a special register called the programme counter or PC, initially set at zero (or to the location of the start of the programme). As programmes run sequentially, the PC will normally have an increment function.</p> <p>To fetch an instruction from memory, we transfer the contents of the programme counter to another special register, the Memory Address Register or MAR. The MAR can assert an address on to the address bus, which will control where main memory points to. The contents of that memory location will then be available on the data bus. These contents can then be read by a special register, the Memory Buffer Register or MBR.  The contents of the MBR then get transferred to another special register, the Current Instruction Register of CIR. The instruction in the CIR is one of the instructions in the CPUs instruction set. It might be a command like ADD, SUBTRACT, LOAD, etc. We execute this command and then increment the programme counter and run through another cycle. The CPU just keeps doing this\u2026\u2026forever!</p> Fig 27. fetch-decode-execute in blocks . <p>There is a simple algebra we can use to describe what is going on. We use names to refer to registers and memory and we use names [in square brackets] to refer to the contents of the register or memory. We use arrows to indicate the flow of data. S</p> <ul> <li>MAR \u2190 [PC]</li> <li>PC \u2190 [PC] + 1</li> <li>MBR \u2190 [Memory]</li> <li>CIR \u2190 MBR</li> </ul> <p>Next we need to look at how the instruction in the CIR gets executed. </p> <p>The CPU needs to be able to \u201cunderstand\u201d what the instruction means. In reality, there is some decoder logic which looks at the bits in the instruction and activates blocks of digital logic based on what the instruction was. In previous notes, we saw an adder or subtractor. Suppose we said that bit zero of the instruction in the CIR would get hooked up to the subtract button of the ALU. In this case, any instruction with bit 0 = 0 would activate the add functionality. Any instruction with bit zero = 1 would activate the subtract functionality. We can encode a different instruction for every combination of binary digits in the CIR. An eight bit CIR would give us the ability to encode 2<sup>8</sup> or 256 different instructions.</p> <p>An instruction to ADD or SUB (subtract) on its own would have no utility; add what? For most instructions we have to pass the values that we want the instruction to work on, for example, the two numbers we want to add together. The data an instruction will execute against are called operands. For example, the ADD instruction 0x00 might be an instruction which means \u201ctake the two next values in memory after this instruction and add them together\u201d. Or for a multiply instruction, take the value in a general purpose register and successively add the contents of the accumulator that many times.</p> <p>The next question\u2026where do we put the results of a calculation?</p> <p>Many processors support the idea of a special register called the accumulator. This is the key register where we perform calculations or place our results.  </p> Fig 28. ALU support . <p>Alternatively, some processors have many general purpose registers. An ADD instruction might be specifically designed to take the contents of two specified general purpose registers and put the result in the accumulator. Your smartphone and the ubiquitous Raspberry Pi both use ARM processors, these have 16/32 registers which can be used for almost any purpose. </p> <p>We have already looked at simple models of the ALU and there isn\u2019t that much more to add. We know that the ALU has control inputs to determine what functions it will run (e.g. ADD, SUB, MULT, DIV). We also know that it needs inputs to provide it with operands (Fig 29). It also has to have a way of getting the results out. In our examples, we show all these separately. In real processors, there is great variation in how this is done. </p> Fig 29. ALU signals . <p>In reality, although we might have three internal busses connecting to the ALU, there is only going to be one data bus in the computer, which everything shares. At some point, the data for Bus A and B will have to come into the processor and the data from Bus C will have to be written back to memory. Our control logic orchestrates all this via a device called a multiplexor. On the diagram (Fig 30), why do we show only two select lines?</p> Fig 30. A 4:1 mux . <p>The final components we need to have before we can have a working data bus is the buffer or tri-state. Only one device can put information on a data or address bus, if more than one device asserts the bus, there will be chaos! We have a component called a tri-state which is like an electronic switch. When you need to connect a device to the data bus, you enable the tri-states which connects each line in the bus to the device. When you no longer want to connect to the bus, you disable the tri-state. </p> Fig 31. Tristate . <p>So now let\u2019s bring together everything we have learned, to sketch a fully working CPU, with external connectivity.</p> Fig 32. Summary . <p>With a real CPU and computer - We have a range of busses interconnecting the internal components within the CPU.  - The internal data bus within the CPU would allow a vale to be transferred to/from the accumulator to/from almost any register.  - The MBR is bi-directional, we can either read data off the data bus or write data to the data bus, based on the address asserted in the MAR. - The external data, address and control bus are shared amongst all the components of the computer. - Everything is orchestrated by the control unit, both inside the CPU and via the external control bus. - There will be buffering between each internal component and the internal data bus. Only one internal component will be able to write to the internal data bus at a time. This is determined by the control unit. -   Multiplexor/De-multiplexor blocks will be used to enable multiple devices to share the same busses -   There will also be buffering between memory chips and the data bus. Complex encoder/decoder logic allows the correct memory chips to be selected and the correct rows/columns activated.</p> <p>That\u2019s it for simple CPU design. We now having a working central processor. Keep in mind though, I have made this as simple as possible, with the minimum components to describe a workable system. We have only scratched the surface. A real CPU has a great deal more complexity and has many more add-ons to increase performance. However, if you can follow what we have covered to date, the add-ons will all make sense! </p>"},{"location":"h/","title":"Enhancing the CPU","text":"<p>Even after 50 years, the principle of operation for an Intel or AMD CPU should be recognisable from the simple descriptions in these notes up to now. To finish this technology introduction, I want to talk about some of the enhancements that have been used to speed up and generally improve the performance of processors.</p> <p>And then the law of unintended consequences kicks in. In recent years, some of the most intractable vulnerabilities in PC hardware were introduced by these enhancements.</p>"},{"location":"h/#floating-point-calculations","title":"Floating Point Calculations","text":"<p>As we have described a CPU, it is a general purpose machine for calculating numbers and manipulating numbers which could represent things like characters. You should have covered ASCII and Unicode in one of my other modules, if you don\u2019t remember, do an internet search on those terms now. However the registers in a CPU are of a fixed size; in modern processors either 32 or 64 bits wide. That means the smallest and biggest numbers we can describe are limited by the size of the register. </p> <ul> <li>What is the biggest integer you can describe in a 32 bit register?  </li> <li>What is the biggest signed integer you can describe in a 32 bit register?</li> </ul> <p>If you don\u2019t remember, do an internet search on those terms now.</p> <p>You should understand scientific notation, if you are rusty, revise independently. If I want to express the number one thousand in scientific notation, I can write it as 1 x 10<sup>3</sup>. If I want to write 1/1000 in scientific notation I can write it as 1 x 10<sup>-3</sup>. Before we go on, make sure you remember all this! If you don\u2019t remember, do an internet search on those terms now. The principle of scientific notation is that we split a number into a fraction and an exponent.</p> Table 6. Floating point examples . <p>We can describe very big and very small numbers in the computer in exactly the same way. Imagine we have a 32 bit register to describe numbers; we call this single precision floating point. We could store the fraction part of the number in the first 24 bits and the exponent in the last 8. The exponent must be signed so only 7 digits could be used to represent numbers and our exponent could range from -127 to +128 (why?). Double precision floating point uses 64 bits where 53 digits are used for the fraction and 11 bits are used for the exponent. Giving one bit for the sign, that gives us an exponent range of -1022 to + 1023. We can now describe very large and very small numbers.</p> <p>The floating point examples given here refer to the IEEE version of floating point. There are variations out there and you may find floating point data which does not match the description above!</p>"},{"location":"h/#co-processors","title":"Co-processors","text":"<p>But a CPU is a general purpose computing device, it was never designed to cope with these very large numbers. It was designed for signed integer values, not floating point. Early PCs did their floating point calculations in a slow and cumbersome way. The chip manufacturers developed special chips specifically to do floating point; these were called co-processors. So for example, the 8086 processor could have an 8087 co-processor installed beside it to enhance its number crunching capabilities.</p> <p>By the 1990s, the chip manufacturers had begun to integrate the co-processor onto the same silicon die as the processor. From the 80486DX onwards, the processor in most personal computers has included the co-processor. If you check the block diagram of any modern CPU, your will find a floating-point unit.</p>"},{"location":"h/#cache","title":"Cache","text":"<p>Once microprocessor clock speeds went faster than 16Mhz (around 1990!) we ran into a performance mismatch; CPUs became faster than main memory. The architecture of main memory is called Dynamic Random Access Memory or DRAM and it has the characteristics of being cheap with a low component count per bit. Each bit is stored as charge on a capacitor, with as little as two transistors controlling the charge. The downside is that DRAM is relatively slow. We could use Static RAM or SRAM, This looks like the memory we built with D Flip Flops, but it has a very high component count per bit and is thus very expensive, takes a great deal of space on the silicon, uses a lot of power, etc. There was no easy fix for this, there is still no good solution today. </p> <p>The way we get around the problem is to have a very large, slow, cheap main memory using DRAM. We then have a much smaller fast expensive cache memory using SRAM. Most programmes run sequentially, one instruction following the other. When we have to read an instruction from memory, we could just as easily read a full kilobyte of instructions from main memory to cache. When we need the next instruction from memory, instead of making a slow call to main memory, we can make a call to the much faster cache memory. This is an instruction cache. Similarly, if we are going to operate on a piece of data, there is a really good chance that the next piece of data we are going to need is right beside the one we just used. We call this spatial locality. If we have previously used a piece of data, there is a high chance we will use it again. This is called temporal locality. The fact that we can prove both of these things to be statistically predictable gives us the ability to size and design our cache and the algorithms to decide how to operate it. This is a data cache. </p> <p>When we look for an instruction in cache and it is there, great! We call that a cache hit and it means that we are operating at maximum efficiency. If not, we have a big delay seeking the data from main memory. This process might be 1,000 times slower than getting the information from the cache. We call this a cache miss.</p> <p>We will look at cache memory, DRAM and SRAM in more detail later in the course.</p>"},{"location":"h/#pipelining","title":"Pipelining","text":"<p>In our simple CPU models, an instruction ripples through the registers of the CPU, taking quite a few clock cycles to complete. Why not break up the activities required by an instruction into a number of steps and do each of these steps concurrently? Consider the steps an ordinary instruction might make (Fig 7). </p> Table 7. Flow of a single instruction . <p>Now suppose we run one instruction after another through the stages. As one as an instruction had finished a stage, the next instruction can use the subsystems required by that stage!</p> Table 8. Pipeline flow of a single instruction . <p>At clock cycle 1 - Execute instruction 1. The first part of the cycle is the instruction fetch. </p> <p>At clock cycle 2 - Instruction 1 can move to the decoder - Execute instruction 1. The first part of the cycle is the instruction fetch. </p> <p>At clock cycle 3  - Instruction 1 can move to reading operands from memory - Instruction 2 can move to the decoder - Execute instruction 3. The first part of the cycle is the instruction fetch. </p> <p>By the fifth clock cycle, 5 instructions are being simultaneously executed through hardware designed only for single processing. As you can imagine, this can massively enhance performance. A modern Intel processor may have up to 20 stages in its pipeline.</p>"},{"location":"h/#logical-processors","title":"Logical Processors","text":"<p>Taking this idea a little further, we can get a CPU to act like it is two logical CPUs. Two separate threads of execution can be multiplexed into the same CPU. Intel calls this technology hyperthreading. If you look at task manager on your laptop and check the performance tab, you'll see sockets, cores and logical processors.</p> <ul> <li>Socket: the physical silicon on the motherboard, a single chip on most laptops, multiple on servers.</li> <li>Core: inside that single chip, we can have multiple CPUs or cores which can share resources on the chip, like cache memory.</li> <li>Logical Processors: using a technology like hyperthreading, give each physical core two threads of execution.</li> </ul> <p>To be clear, two threads of execution into a single core does not give the same performance as two cores. Often it's quoted as a 40% increase in performance. Particularly in the world of virtualization hosts, we need to be cognizant of this. There are times when a hypervisor might be optimised by switching off hyperthreading.</p>"},{"location":"i/","title":"Alternative Processor Paradigms","text":"<p>So far on this module we have looked at the technologies used in workstations, servers and laptops.  For the past forty years, this market has been controlled by Intel and a few additional players like AMD. </p> <p>Reduced Instruction Set Computers (RISC) is a processor design using simple hardware and highly optimized instruction sets. They are faster per instructions, physically smaller and simpler and as a result, more power efficient. </p>"},{"location":"i/#risc","title":"RISC","text":"<p>When we look into the actual execution of instructions in a processor, we can identify empirically (by experimentation and real data) which instructions are used most, which take most processor time. A processor is a number cruncher, so you would guess that arithmetic logic instructions would be executed most? </p> <p>Wrong! </p> <p>A processor spends more time shifting data in and out of memory than doing anything else. The second most common thing for it to do is to control the flow of program execution. Optimization techniques such as pipelining and caching are intended to optimize this.  </p> <p>Reduced Instruction Set or RISC processors were first defined in a 1980 paper by Patterson and Ditzel [1] and early experimentation with RISC in Berkeley exposed some of the characteristics of this model.</p> <ul> <li>Processors were kept as simple as possible with fixed instruction sets or fixed length. </li> <li>Instructions which process data only operate on registers, not on memory, speeding up and simplifying processing.</li> <li>There are many registers, typically thirty two. This was far in excess of the handful of registers in a typical CISC processor.</li> <li>Instruction decoding is hard wired. CISC processors were so complex, they required microcode in the core to assist decoding.</li> <li>There is a concentration of optimization strategies like pipelines.</li> </ul> <p>The emergent properties of this strategy were that die sizes were much smaller, with fewer transistors in a smaller silicon chip. This resulted in less power consumption, a shorter development time, and for a range of complex reasons, better performance and reduced costs. \u2003</p>"},{"location":"i/#advanced-risc-machines-arm-processors","title":"Advanced RISC Machines (ARM) processors","text":"<p>Acorn Computer Ltd. (Cambridge, England) developed the first commercial RISC chip in the mid-1980s, the ARM processor [2]. They have an interesting business model. They develop the instruction sets, tools and specifications and then license the production of chips to large silicon foundries. </p> <p>ARM is now the most widely used instruction set worldwide. Early chips shipped were 32 bit; but from 2011, a 64 bit version (ARMv8) has been available. The ARM contains all the components for a computer on a single silicon die; it is therefore a System on a Chip or SoC. The instruction set design is what we would expect from RISC. Instructions are simple and most instructions execute in a single clock-cycle. If you want to work with an ARM based system, a Raspberry PI is a cheap and easy solution. You can load a full copy of Linux (the standard is a derivative of Debian Linux). </p> <p>The cost of a board like the Raspberry Pi 5 is about \u20ac50, depending on memory etc.</p>"},{"location":"i/#microcontrollers","title":"Microcontrollers","text":"<p>A vast amount of devices need simple, low power (in terms of both compute and consumption) controllers. Since the late 1970s controllers such as the Peripheral Interface Controller (PIC) have been used for embedded systems and small systems control.  A modern PIC will be programmable with on board flash memory. Typically a PIC will be programmed in Basic, C or C++ and will have simple and widely available tools, code examples and application notes. </p> <p>In machine code, there are a very limited number of instructions (40-80) and they are all fixed in length. There is one accumulator register (W0) which holds the results of all the calculations. Oddly, the RAM used for data is also used for storing temporary values etc. This memory is used like it was a series of registers which map into RAM. Memory is mostly 8 bit although this varies with higher end PICs.</p> <p>Programs are stored in a separate memory location, normally in flash RAM, so they are non-volatile. </p> <p>There is a stack, but it is implemented in the hardware, separately.</p> <p>[1] Patterson, D.A. and Ditzel, D.R., 1980. The case for the reduced instruction set computer. ACM SIGARCH Computer Architecture News, 8(6), pp.25-33.</p> <p>[2] Furber, S.B., 2000. ARM system-on-chip architecture. Pearson Education.</p>"},{"location":"i/#graphics-processor-units-gpu","title":"Graphics Processor Units (GPU)","text":"<p>Original video cards began as devices without onboard calculation capability, beyond simple memory operations.  The arcade game industry pushed the limit of these techniques. The first dedicated display processors began to emerge in the 1980s.  From the 1990s, cards were developed to take load off the CPU and perform graphics operations indendently. Graphics Processor Units were deleloped as video cards for complex rendering, typically for game technology. </p> <p>I'm not going to dig in to them further here. If you are interested in AI/ML then do some independent reading now. In particular, read about nVIDIA technology and CUDA.</p>"},{"location":"k/","title":"Automation and I/O","text":"<p>As PICs are commonly used for automation, we have a whole new set of terms to describe how to interface with the outside world. We have to be careful here. A PIC will require signals with particular characteristics, such as high and low voltage. The process you are working with (whatever you are automating or controlling) may use completely different signal levels. You may need to come up with conditioning circuitry to interface the two. </p> <p>For example. I like to use Raspberry Pi as a basic controller and Internet of Things (IoT) device. It has a general purpose input/output interfae (GPIO) for digital singals. But it expects +/- 3.3VDC and almost any higher voltage will do physical damage to the board. I cannot connect an RS232 interface without chaning volatge levels. I may need a relay board to switch voltage/power. Look up electromechanical relays, in this context they are logic controlled switches.  </p>"},{"location":"k/#digital-input-di","title":"Digital Input (DI)","text":"<p>A binary signal, a high voltage is one and a low voltage is zero. High and low voltage are defined by the circuit, typically 5VDC and 0VDC.</p>"},{"location":"k/#digital-output-do","title":"Digital Output (DO)","text":"<p>A binary signal, a high voltage is one and a low voltage is zero. High and low voltage are defined by the circuit, typically 5VDC and 0VDC. The current output is very limited, so if you are switching lights or a motor, you will need relays or additional switching electronics.</p>"},{"location":"k/#analogue-input-ai","title":"Analogue Input (AI)","text":"<p>An analogue to digital converter (ADC) take real world variable voltages and converts them into digital signals. The two important points are, how frequently does the ADC sample (for example, a sound card may sample at 44kHz) and how many bits per sample (8 bits per sample would allow for 256 discrete levels).</p>"},{"location":"k/#analogue-output-ao","title":"Analogue Output (AO)","text":"<p>A digital to analogue converter (DAC) takes a digital numeric value and converts it into an analogue voltage level. As with analogue inputs, it will do so at a frequency and in certain steps, depending on the resolution of the DAC.</p>"},{"location":"k/#pulse-input-pi","title":"Pulse Input (PI)","text":"<p>Many processes emit pulses. For example, one way of measuring speed and distance is to have a magnet at one point on a wheel. As it passes a sensor, the sensor emits a pulse. Counting these pulses in a fixed time (e.g. one second) allows the frequency of revolution of the wheel to be estimated.</p>"},{"location":"k/#pulse-output-po","title":"Pulse Output (PO)","text":"<p>Some processes may be controlled by a pulsed output, where either the number of duration of pulses controls an actuator. Stepper motors (used in robotics) are an example of a device which could be controlled by pulses. </p>"}]}